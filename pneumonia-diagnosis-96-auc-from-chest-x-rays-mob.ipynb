{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23812,"sourceType":"datasetVersion","datasetId":17810}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n# THEN FEEL FREE TO DELETE THIS CELL.\n# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# NOTEBOOK.\nimport kagglehub\npaultimothymooney_chest_xray_pneumonia_path = kagglehub.dataset_download('paultimothymooney/chest-xray-pneumonia')\n\nprint('Data source import complete.')\n","metadata":{"id":"6sRIz6M_s08U","outputId":"3f3f6ebc-6195-498d-9208-cfc411dd5203"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"muDmvdZ8s08Y","outputId":"d1e9d492-102e-44c6-e6d6-23114e6a318e"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Pneumonia Detection using Chest X-Ray Images (Pneumonia)\n\n Classify chest X-ray images as either Pneumonia or Normal.","metadata":{"id":"2_odxCHruXZM"}},{"cell_type":"markdown","source":"# ðŸ“‚ 2. Data Loading and Structure Check\n\nIn this section, we:\n- Import libraries\n- Mount Google Drive (if needed)\n- Inspect the dataset folder structure\n- Count and visualize image samples\n- Check for corrupt/missing files\n- Prepare for train/val/test splits if not already present\n","metadata":{"id":"yLJ-iAcSulZF"}},{"cell_type":"markdown","source":"##  2.1 Import Required Libraries\n","metadata":{"id":"d7FRVK5LunOn"}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom tqdm import tqdm\nfrom PIL import Image\nimport shutil\nimport random\n\nfrom sklearn.model_selection import train_test_split\n","metadata":{"id":"tHBSgWrDuXGV"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  2.2 Define Dataset Directory\n","metadata":{"id":"xsr1Rdw8uqcL"}},{"cell_type":"code","source":"# Set this path to your actual dataset directory\ndataset_dir = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"\n\n# Check subdirectories\nfor split in ['train', 'val', 'test']:\n    split_path = os.path.join(dataset_dir, split)\n    if os.path.exists(split_path):\n        print(f\"âœ… Found: {split_path}\")\n    else:\n        print(f\"âš ï¸ Missing: {split_path}\")\n","metadata":{"id":"SleZG8ajtK-U","outputId":"db4a6eff-4c67-4519-c279-808ea38e16ec"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  2.3 Count Images Per Class in Each Folder\n","metadata":{"id":"jA6VeKx5u7uM"}},{"cell_type":"code","source":"def count_images(path):\n    class_counts = {}\n    for cls in os.listdir(path):\n        cls_path = os.path.join(path, cls)\n        if os.path.isdir(cls_path):\n            num_images = len(os.listdir(cls_path))\n            class_counts[cls] = num_images\n    return class_counts\n\nfor split in ['train', 'val', 'test']:\n    print(f\"\\nðŸ“Š {split.upper()} Split:\")\n    print(count_images(os.path.join(dataset_dir, split)))\n","metadata":{"id":"Z7r909DJu5WM","outputId":"b3abadd3-81b2-4750-dd0d-8de71188bee6"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  2.4 Visualize Sample Images from Each Class\n","metadata":{"id":"Q4NCL9LLvCjv"}},{"cell_type":"code","source":"def show_sample_images(base_path, class_name, num=5):\n    class_path = os.path.join(base_path, class_name)\n    sample_imgs = random.sample(os.listdir(class_path), num)\n\n    plt.figure(figsize=(15, 5))\n    for i, img_name in enumerate(sample_imgs):\n        img = Image.open(os.path.join(class_path, img_name))\n        plt.subplot(1, num, i+1)\n        plt.imshow(img.convert(\"L\"), cmap='gray')\n        plt.title(class_name)\n        plt.axis('off')\n    plt.suptitle(f\"ðŸ” {class_name} Sample Images\")\n    plt.show()\n\n# Example: Display 5 samples from training Normal and Pneumonia\nshow_sample_images(os.path.join(dataset_dir, 'train'), 'NORMAL', 5)\nshow_sample_images(os.path.join(dataset_dir, 'train'), 'PNEUMONIA', 5)\n","metadata":{"id":"_xjD5LP8u-z7","outputId":"96e519ab-caca-4ec3-cbe4-23e5916b429a"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  2.5 Check for Corrupted Images\n","metadata":{"id":"E7uRbYw9vU5l"}},{"cell_type":"code","source":"def check_corrupt_images(directory):\n    corrupt_count = 0\n    for root, _, files in os.walk(directory):\n        for f in tqdm(files, desc=f\"Checking {root}\"):\n            try:\n                img_path = os.path.join(root, f)\n                img = Image.open(img_path)\n                img.verify()\n            except:\n                print(f\"âŒ Corrupt: {img_path}\")\n                corrupt_count += 1\n    print(f\"\\nðŸ§¹ Total Corrupt Images: {corrupt_count}\")\n\ncheck_corrupt_images(dataset_dir)\n","metadata":{"id":"NlCPlJ3pvFRs","outputId":"2c9ccbe9-b66c-4228-bf5d-76bf6db19f2e"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ“Š 3. Exploratory Data Analysis (EDA)\n\nEDA helps us understand the class distribution, sample quality, and visual patterns in images.\nWe will explore:\n- ðŸ“¦ Image count by class per split\n- ðŸ“ Image dimension distributions\n- ðŸ“¸ Visual examples (normal vs pneumonia)\n- ðŸŒˆ Pixel intensity distributions\n- ðŸ§  Image similarity (PCA & TSNE later)\n","metadata":{"id":"w0W378_RwjHg"}},{"cell_type":"markdown","source":"## ðŸ”¹ 3.1 Class Distribution per Split\n","metadata":{"id":"gw50rPgfwtOk"}},{"cell_type":"code","source":"splits = ['train', 'val', 'test']\nclass_dist = {}\n\nfor split in splits:\n    path = os.path.join(dataset_dir, split)\n    class_counts = {cls: len(os.listdir(os.path.join(path, cls))) for cls in os.listdir(path)}\n    class_dist[split] = class_counts\n\ndf_dist = pd.DataFrame(class_dist)\ndf_dist.T.plot(kind='bar', stacked=True, figsize=(10,6), colormap=\"Set3\")\nplt.title(\" Class Distribution per Split\")\nplt.ylabel(\"Number of Images\")\nplt.xticks(rotation=0)\nplt.grid(axis='y')\nplt.show()\n","metadata":{"id":"uqPHODRbwaKN","outputId":"6b93d391-11d8-4e23-9e74-c6453ff3efdf"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  3.2 Image Size Distribution\n","metadata":{"id":"LvHiU0yUwzIg"}},{"cell_type":"code","source":"image_shapes = []\nbase_path = os.path.join(dataset_dir, 'train')\n\nfor cls in os.listdir(base_path):\n    class_path = os.path.join(base_path, cls)\n    files = random.sample(os.listdir(class_path), 200)  # sample 200 for speed\n    for f in files:\n        img = Image.open(os.path.join(class_path, f))\n        image_shapes.append(img.size)\n\nwidths, heights = zip(*image_shapes)\nplt.figure(figsize=(10,5))\nsns.histplot(widths, color=\"skyblue\", label='Width', kde=True)\nsns.histplot(heights, color=\"orange\", label='Height', kde=True)\nplt.title(\" Image Size Distribution\")\nplt.xlabel(\"Pixels\")\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"id":"ZZVpexIWwv38","outputId":"a797a99f-a0fb-4d38-b65a-a95eb265e41b"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  3.3 Image Brightness/Intensity Distribution\n","metadata":{"id":"dxhIgYwCw8BV"}},{"cell_type":"code","source":"def get_pixel_stats(image_paths):\n    means = []\n    stds = []\n\n    for img_path in tqdm(image_paths[:500]):\n        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        img = cv2.resize(img, (224, 224))\n        means.append(np.mean(img))\n        stds.append(np.std(img))\n    return means, stds\n\ntrain_path = os.path.join(dataset_dir, 'train')\nall_image_paths = []\n\nfor cls in os.listdir(train_path):\n    cls_path = os.path.join(train_path, cls)\n    all_image_paths.extend([os.path.join(cls_path, f) for f in os.listdir(cls_path)])\n\nmeans, stds = get_pixel_stats(all_image_paths)\n\nplt.figure(figsize=(12, 5))\nsns.histplot(means, bins=30, color='navy', kde=True)\nplt.title(\" Mean Pixel Intensity Distribution\")\nplt.xlabel(\"Mean Intensity (0-255)\")\nplt.grid(True)\nplt.show()\n","metadata":{"id":"qSK38fMzw9Of","outputId":"076d8523-43f6-4aed-e361-ac4b076fb6cc"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  3.4 Visual Comparison: NORMAL vs PNEUMONIA\n","metadata":{"id":"ehJ8N4kNxDGD"}},{"cell_type":"code","source":"def plot_class_samples(split='train', num=5):\n    plt.figure(figsize=(15, 4))\n    for i, cls in enumerate(['NORMAL', 'PNEUMONIA']):\n        class_path = os.path.join(dataset_dir, split, cls)\n        sample_files = random.sample(os.listdir(class_path), num)\n        for j, file in enumerate(sample_files):\n            img = Image.open(os.path.join(class_path, file))\n            plt.subplot(2, num, i*num + j + 1)\n            plt.imshow(img.convert('L'), cmap='gray')\n            plt.title(f\"{cls}\")\n            plt.axis('off')\n    plt.suptitle(\" Sample Images: NORMAL vs PNEUMONIA\", fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\nplot_class_samples('train', 5)\n","metadata":{"id":"EyHz7F8KxEcO","outputId":"e3c57286-d8e4-49e7-e551-f8cf1f62c5fe"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  3.5 Optional: Average Image Per Class\nHelps identify texture/pattern differences.\n","metadata":{"id":"9JhPdBmHxd_0"}},{"cell_type":"code","source":"def average_image(path):\n    images = []\n    for f in random.sample(os.listdir(path), 300):\n        img = cv2.imread(os.path.join(path, f), cv2.IMREAD_GRAYSCALE)\n        img = cv2.resize(img, (224, 224))\n        images.append(img)\n    return np.mean(images, axis=0)\n\nnormal_avg = average_image(os.path.join(dataset_dir, 'train/NORMAL'))\npneumonia_avg = average_image(os.path.join(dataset_dir, 'train/PNEUMONIA'))\n\nplt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nplt.imshow(normal_avg, cmap='gray')\nplt.title(\"Average: NORMAL\")\nplt.axis('off')\n\nplt.subplot(1,2,2)\nplt.imshow(pneumonia_avg, cmap='gray')\nplt.title(\"Average: PNEUMONIA\")\nplt.axis('off')\nplt.suptitle(\" Class-wise Average X-rays\", fontsize=14)\nplt.show()\n","metadata":{"id":"JUkqJ8qkxGx8","outputId":"b9478ac3-7a55-4554-8600-139ada00d759"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ§¼ 4. Image Preprocessing & Augmentation\n\nTo improve model generalization and performance, we need a strong preprocessing pipeline.\n\n### Key Preprocessing Steps:\n- ðŸ–¼ Resize to standard dimensions (224x224)\n- ðŸŒˆ Convert to 3 channels (RGB)\n- ðŸ§® Normalize pixel values [0-1]\n- ðŸ§ª Augment training set:\n  - Random Zoom, Shift, Rotate, Flip\n  - Brightness & Contrast Tweaks\n","metadata":{"id":"IQ70ox_Cx-mk"}},{"cell_type":"code","source":"! pip install albumentations","metadata":{"id":"j0Vflp8xyHX5","outputId":"98a30299-4100-4203-f784-d34ffba2f141"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“¦ Libraries\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport albumentations as A\nfrom albumentations.core.composition import OneOf\nfrom albumentations import transforms as T\nfrom albumentations import geometric as G\nfrom albumentations import blur as B\nfrom albumentations import crops as C\n\nfrom tensorflow.keras.utils import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img","metadata":{"id":"zsRd_eSYxgjj"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ”¹ 4.1 Define Albumentations Transform (for Training)\n","metadata":{"id":"Y3n6PsrsyBfM"}},{"cell_type":"code","source":"albumentation_train = A.Compose([\n    A.Resize(224, 224),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.5),\n    A.Rotate(limit=10, p=0.3),\n    A.ZoomBlur(p=0.2),\n    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.4),\n    A.OneOf([\n        A.GaussianBlur(p=0.2),\n        A.MotionBlur(p=0.2),\n        A.MedianBlur(blur_limit=3, p=0.2)\n    ], p=0.3),\n    A.Normalize(),  # mean=0, std=1\n])\n","metadata":{"id":"uooo-zRhyhxc","outputId":"133bb09a-3a9c-4b66-f8d4-1e660cb52689"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  4.2 Keras Generator with Albumentations (Custom Pipeline)\n","metadata":{"id":"PuX2sOYryjJ2"}},{"cell_type":"code","source":"import tensorflow as tf\nclass CustomDataGen(tf.keras.utils.Sequence):\n    def __init__(self, image_paths, labels, transform, batch_size=32, shuffle=True):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(np.ceil(len(self.image_paths) / self.batch_size))\n\n    def __getitem__(self, idx):\n        batch_paths = self.image_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        images = []\n        for path in batch_paths:\n            image = cv2.imread(path)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            augmented = self.transform(image=image)\n            images.append(augmented['image'])\n\n        return np.array(images), np.array(batch_labels)\n\n    def on_epoch_end(self):\n        if self.shuffle:\n            zipped = list(zip(self.image_paths, self.labels))\n            random.shuffle(zipped)\n            self.image_paths, self.labels = zip(*zipped)","metadata":{"id":"dFmn-jO3ylTX"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ”¹ 4.3 Create Data Loaders for Train, Val, and Test\n","metadata":{"id":"HyZ9wZVKynFw"}},{"cell_type":"code","source":"def get_image_paths_and_labels(folder):\n    classes = sorted(os.listdir(folder))\n    class_map = {cls: idx for idx, cls in enumerate(classes)}\n    image_paths = []\n    labels = []\n\n    for cls in classes:\n        class_path = os.path.join(folder, cls)\n        for f in os.listdir(class_path):\n            image_paths.append(os.path.join(class_path, f))\n            labels.append(class_map[cls])\n\n    return image_paths, labels\n\ntrain_dir = os.path.join(dataset_dir, 'train')\nval_dir = os.path.join(dataset_dir, 'val')\ntest_dir = os.path.join(dataset_dir, 'test')\n\ntrain_paths, train_labels = get_image_paths_and_labels(train_dir)\nval_paths, val_labels = get_image_paths_and_labels(val_dir)\ntest_paths, test_labels = get_image_paths_and_labels(test_dir)\n\n# Train uses augmentation, validation/test use resize+normalize only\nval_aug = A.Compose([A.Resize(224, 224), A.Normalize()])\n\ntrain_loader = CustomDataGen(train_paths, train_labels, albumentation_train, batch_size=32)\nval_loader = CustomDataGen(val_paths, val_labels, val_aug, batch_size=32, shuffle=False)\ntest_loader = CustomDataGen(test_paths, test_labels, val_aug, batch_size=32, shuffle=False)\n","metadata":{"id":"iJkOYH_1yqrk"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## âœ… Summary of Step 4:\n\n- Custom `Albumentations` augmentations are **stronger than traditional Keras** generators.\n- Data is fed in real-time via a `Sequence` class.\n- Highly modular: easy to plug into model training.\n","metadata":{"id":"fTD8Z7Sby2ec"}},{"cell_type":"markdown","source":"# ðŸ§ª 5. Train-Validation-Test Split (Advanced)\n\nProper splitting ensures:\n- âœ… No data leakage\n- âœ… Balanced class distribution\n- âœ… Generalization across unseen patients\n","metadata":{"id":"Mu9qHMU8zxml"}},{"cell_type":"markdown","source":"## 5.1 Strategy: Patient-wise Stratified Splitting\nIn medical datasets, multiple X-rays from the same patient can cause data leakage if they appear in both training and testing.","metadata":{"id":"zO1MJTGhzyxu"}},{"cell_type":"code","source":"import os\nfrom collections import Counter\n\ndef count_images(path):\n    count = {}\n    for label in ['NORMAL', 'PNEUMONIA']:\n        label_dir = os.path.join(path, label)\n        count[label] = len(os.listdir(label_dir))\n    return count\n\ntrain_counts = count_images(train_dir)\nval_counts = count_images(val_dir)\ntest_counts = count_images(test_dir)\n\nprint(\"âœ… Train:\", train_counts)\nprint(\"âœ… Val:\", val_counts)\nprint(\"âœ… Test:\", test_counts)\n","metadata":{"id":"yvK8F3eRy3Fb","outputId":"f7dda53f-23a4-4f96-d4e5-2fbfdb58e011"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.2 If Manual Split is Needed from Raw Folder\nUse Stratified Split + Patient Metadata (if available):","metadata":{"id":"-codk6Y40ZB3"}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# ðŸ“ Define paths\ntrain_dir = \"/kaggle/input/chest-xray-pneumonia/chest_xray/train\"\nval_dir = \"/kaggle/input/chest-xray-pneumonia/chest_xray/val\"\ntest_dir = \"/kaggle/input/chest-xray-pneumonia/chest_xray/test\"\n\n# ðŸ” Define ImageDataGenerators\ntrain_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20,\n                                   zoom_range=0.2, horizontal_flip=True)\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n# ðŸŽ¯ Load from directory\ntrain_gen = train_datagen.flow_from_directory(\n    train_dir, target_size=(224, 224), batch_size=32,\n    class_mode=\"binary\", shuffle=True)\n\nval_gen = val_datagen.flow_from_directory(\n    val_dir, target_size=(224, 224), batch_size=32,\n    class_mode=\"binary\", shuffle=False)\n\ntest_gen = test_datagen.flow_from_directory(\n    test_dir, target_size=(224, 224), batch_size=32,\n    class_mode=\"binary\", shuffle=False)\n","metadata":{"id":"6ztkSn2Xz9mU","outputId":"72da7231-8b62-48c6-bbbd-9dabf0ffcf1c"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  5.3 Class Distribution Plot (Check for Imbalance)\n","metadata":{"id":"D35GypEC25Am"}},{"cell_type":"code","source":"def count_images_in_dir(base_dir):\n    categories = ['NORMAL', 'PNEUMONIA']\n    data_count = {}\n    for category in categories:\n        data_count[category] = {\n            'train': len(os.listdir(os.path.join(base_dir, 'train', category))),\n            'val': len(os.listdir(os.path.join(base_dir, 'val', category))),\n            'test': len(os.listdir(os.path.join(base_dir, 'test', category)))\n        }\n    return data_count\n\n# ðŸ“ Base dataset directory\nbase_dir = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"\n\n# ðŸ“ˆ Count images\ndata_count = count_images_in_dir(base_dir)\n\n# ðŸ” Plotting\nlabels = list(data_count.keys())\nsplits = ['train', 'val', 'test']\ncolors = ['#4CAF50', '#FFC107', '#2196F3']\n\nx = range(len(splits))\nbar_width = 0.35\n\nfor idx, label in enumerate(labels):\n    counts = [data_count[label][split] for split in splits]\n    plt.bar([i + idx * bar_width for i in x], counts, width=bar_width, label=label, color=colors[idx])\n\nplt.xlabel(\"Dataset Split\")\nplt.ylabel(\"Number of Images\")\nplt.title(\" Class Distribution per Split\")\nplt.xticks([r + bar_width / 2 for r in x], splits)\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()","metadata":{"id":"kIF3SSCO1_Hc","outputId":"5846e905-4a63-4d49-f322-34ba6d2ba7b1"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.4: Rebuild Val Set from Training Data\nSince we have 5,216 training images, we can extract a proper validation set from that using Stratified Split:\n\nâœ… We'll create new train_gen and val_gen from original train_dir only.","metadata":{"id":"k4F9lJQCBmV-"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport pandas as pd\nimport shutil\nimport os\n\n# ðŸ” 1. Get all image paths and labels from original train_dir\ndef get_image_paths_labels(base_dir):\n    categories = ['NORMAL', 'PNEUMONIA']\n    data = []\n\n    for label in categories:\n        path = os.path.join(base_dir, label)\n        for fname in os.listdir(path):\n            data.append({\n                'path': os.path.join(path, fname),\n                'label': label\n            })\n\n    return pd.DataFrame(data)\n\ndf = get_image_paths_labels(\"/kaggle/input/chest-xray-pneumonia/chest_xray/train\")\n\n# ðŸŽ¯ 2. Stratified split: 85% train, 15% val\ntrain_df, val_df = train_test_split(df, test_size=0.15, stratify=df['label'], random_state=42)\n\nprint(f\"Train: {len(train_df)}, Val: {len(val_df)}\")\n","metadata":{"id":"KYDeIHf9BrJv","outputId":"1ebbf62c-b021-41b3-e457-019cc5497ff5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Train:\", train_df['label'].value_counts())\nprint(\"Val:\", val_df['label'].value_counts())\n\n","metadata":{"id":"ctD-MMwaCRwy","outputId":"c46fe440-da05-46dd-af65-cb42ff2392c3"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\n\n# ðŸ—‚ï¸ Define base output folders\nbase_output = \"chest_xray_split\"\nos.makedirs(base_output, exist_ok=True)\n\ndef copy_images(df, split_name):\n    for label in ['NORMAL', 'PNEUMONIA']:\n        split_folder = os.path.join(base_output, split_name, label)\n        os.makedirs(split_folder, exist_ok=True)\n\n    for _, row in df.iterrows():\n        label = row['label']\n        src = row['path']\n        dst = os.path.join(base_output, split_name, label, os.path.basename(src))\n        shutil.copy2(src, dst)\n\n# ðŸš€ Copy images to new structure\ncopy_images(train_df, 'train')\ncopy_images(val_df, 'val')\n\nprint(\"âœ… Images copied to chest_xray_split/train & val\")\n","metadata":{"id":"LQQWP5j8EAsu","outputId":"fb2f5468-df38-442d-b7cf-4acda88ac2d7"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_output = \"chest_xray_split\"\n\ntrain_datagen_new = ImageDataGenerator(rescale=1./255, rotation_range=15, zoom_range=0.1, horizontal_flip=True)\nval_datagen_new = ImageDataGenerator(rescale=1./255)\n\n\ntrain_data = train_datagen_new.flow_from_directory(\n    os.path.join(base_output, 'train'),\n    target_size=(224, 224),\n    class_mode='binary',\n    batch_size=32,\n    shuffle=True\n)\n\nval_data = val_datagen_new.flow_from_directory(\n    os.path.join(base_output, 'val'),\n    target_size=(224, 224),\n    class_mode='binary',\n    batch_size=32,\n    shuffle=False\n)","metadata":{"id":"HE5iDh0hHS__","outputId":"1c2b55f2-0dfd-48c2-e4d9-f0504110a564"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(os.listdir(train_dir))\n","metadata":{"id":"1DVivZzDHrKA","outputId":"d18b9cfb-3229-4935-e0a8-02b6438ddca2"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_gen = ImageDataGenerator(rescale=1./255, rotation_range=15, zoom_range=0.1, horizontal_flip=True)\nval_gen = ImageDataGenerator(rescale=1./255)\n\ntrain_data = train_gen.flow_from_directory(\n    os.path.join(base_output, 'train'),\n    target_size=(224, 224),\n    class_mode='binary',\n    batch_size=32,\n    shuffle=True\n)\n\nval_data = val_gen.flow_from_directory(\n    os.path.join(base_output, 'val'),\n    target_size=(224, 224),\n    class_mode='binary',\n    batch_size=32,\n    shuffle=False\n)\n","metadata":{"id":"rZ_-XvkoEQdM","outputId":"b982bbdf-6521-4e7a-a2c9-1981fb71e4a7"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 6: Model Building using MobileNetV2 (Frozen Base + Custom Classifier)\n\nWe'll use **MobileNetV2**, a lightweight CNN architecture pretrained on ImageNet, and add a custom classification head for Pneumonia detection.\n\nAdvantages:\n- Leverages strong pretrained features.\n- Works well on medical images.\n- Faster training and better generalization on smaller datasets.\n","metadata":{"id":"6Y4TkThf4Z0P"}},{"cell_type":"markdown","source":"## 6.1 Load MobileNetV2 (Frozen Base)\nWe load the MobileNetV2 model **without the top layers** and freeze its weights to use it as a **feature extractor**.\n","metadata":{"id":"zwr-7ASQ4c2c"}},{"cell_type":"code","source":"from tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input\nfrom tensorflow.keras.optimizers import Adam\n\n# ðŸŽ¯ Input shape\ninput_shape = (224, 224, 3)\n\n# ðŸ”’ Base model\nbase_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=Input(shape=input_shape))\n\n# â„ï¸ Freeze the base model layers\nfor layer in base_model.layers:\n    layer.trainable = False\n","metadata":{"id":"UUDVmeE13E8k","outputId":"47597578-af45-4e5f-dc77-0ad05ddf5193"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  6.2 Custom Head for Binary Classification\n\nWe add:\n- `GlobalAveragePooling`: Reduce spatial dims.\n- `Dropout`: Prevent overfitting.\n- `Dense`: Learn task-specific features.\n- `Sigmoid`: Output probability for binary classification.\n","metadata":{"id":"S4z2IR2x4g5M"}},{"cell_type":"code","source":"# ðŸ§  Add custom head\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dropout(0.3)(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.3)(x)\noutput = Dense(1, activation='sigmoid')(x)  # Binary classification\n\n# ðŸ“¦ Create final model\nmodel = Model(inputs=base_model.input, outputs=output)\n","metadata":{"id":"vyj-Lqci4e_d"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  6.3 Compile the Model\n\nWe use:\n- `Adam` optimizer with small learning rate\n- `Binary Crossentropy` for 2-class problem\n- `Accuracy` as the main metric\n","metadata":{"id":"4m85Zk_W4rvt"}},{"cell_type":"code","source":"model.compile(optimizer=Adam(learning_rate=1e-4),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n","metadata":{"id":"VobQdAET4pYF"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Summary of the Model","metadata":{"id":"14KKvJyJEkLY"}},{"cell_type":"code","source":"model.summary()\n","metadata":{"id":"4xseT4cXEh_g","outputId":"cf39d792-8cf3-4dff-d49b-50cecf9147b5"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ðŸ§  Step 6: Transfer Learning with MobileNetV2\n\nWe employ MobileNetV2 as a feature extractor to leverage pre-trained ImageNet knowledge and reduce overfitting risk on medical images.\n\n**Architecture Overview:**\n- âœ… Base: MobileNetV2 (frozen, no top layers)\n- âœ… Head:\n  - GlobalAveragePooling2D\n  - Dense(128, ReLU)\n  - Dropout (30%)\n  - Output Layer: Dense(1, Sigmoid)\n\n**Why This Architecture?**\n- Efficient on small datasets\n- Faster convergence\n- Excellent performance on low-resource environments\n\n**Loss**: Binary Crossentropy  \n**Optimizer**: Adam (LR=1e-4)  \n","metadata":{"id":"7nT-fvEuEoNv"}},{"cell_type":"markdown","source":"# Step 7: Model Training with Data Augmentation, Callbacks & Monitoring\n\nIn this step, weâ€™ll:\n- Enhance training using image augmentation.\n- Prevent overfitting and track performance with callbacks.\n- Train the model efficiently on GPU with live logs.\n\nâœ… Augmentation simulates real-world variations.\nâœ… Callbacks improve generalization and save best model.\n","metadata":{"id":"3dLY-AaM42OF"}},{"cell_type":"markdown","source":"### ðŸ”¹ 7.1 Data Augmentation\n\nWe apply random transformations to make the model more robust to unseen test images and reduce overfitting.\n","metadata":{"id":"p5_qqaUv44E9"}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_aug = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=15,\n    zoom_range=0.1,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_aug = ImageDataGenerator(rescale=1./255)\n","metadata":{"id":"jbpGOKee4tes"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  7.2 Setup Generators","metadata":{"id":"I_woyuQM47tI"}},{"cell_type":"code","source":"train_dir = 'chest_xray_split/train'\nval_dir = 'chest_xray_split/val'\n\ntrain_gen = train_aug.flow_from_directory(\n    train_dir, target_size=(224, 224),\n    batch_size=32, class_mode='binary', shuffle=True\n)\n\nval_gen = val_aug.flow_from_directory(\n    val_dir, target_size=(224, 224),\n    batch_size=32, class_mode='binary', shuffle=False\n)\n","metadata":{"id":"qvcgtd3O45cl","outputId":"667a9d44-89ef-4b33-c558-368057a43b36"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import AUC\n\nmodel.compile(\n    optimizer=Adam(learning_rate=1e-4),\n    loss='binary_crossentropy',\n    metrics=['accuracy', AUC(name='auc')]\n)","metadata":{"id":"JLUqYG4IQN2_"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  7.3 Callbacks Used:\n- **ModelCheckpoint**: Saves best weights only.\n- **EarlyStopping**: Prevents overfitting by stopping early.\n- **ReduceLROnPlateau**: Reduces LR if validation loss stagnates.\n","metadata":{"id":"fArkVrUQ4__O"}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\ncheckpoint_cb = ModelCheckpoint(\n    \"best_final_model.h5\", monitor=\"val_auc\",\n    save_best_only=True, mode=\"max\", verbose=1\n)\n\nearly_cb = EarlyStopping(\n    monitor=\"val_auc\", patience=5,\n    mode=\"max\", restore_best_weights=True\n)\n\nreduce_lr_cb = ReduceLROnPlateau(\n    monitor='val_loss', factor=0.2, patience=3,\n    verbose=1, min_lr=1e-6\n)\n\ncallbacks = [checkpoint_cb, early_cb, reduce_lr_cb]\n","metadata":{"id":"edtmOypJ49Rk"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7.4 Train the Model","metadata":{"id":"SYLLoxxO5H1X"}},{"cell_type":"code","source":"history = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=10,\n    callbacks=callbacks\n)\n","metadata":{"id":"fsjBS_Pa5Bz8","outputId":"a9dbcebc-1d84-4b8b-c0d0-1e194ed9fd5c"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ” Training Progress\n\nWeâ€™ll visualize how well the model learns across epochs.\n","metadata":{"id":"b1f2rhy05Oxs"}},{"cell_type":"code","source":"def plot_history(history):\n    plt.figure(figsize=(12,4))\n\n    # Accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Train Acc')\n    plt.plot(history.history['val_accuracy'], label='Val Acc')\n    plt.title(' Accuracy over Epochs')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    # Loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Val Loss')\n    plt.title(' Loss over Epochs')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\nplot_history(history)","metadata":{"id":"2nLedTXn5J5U","outputId":"89e28d1d-8cd9-4276-f68f-8d7d1ceb7c7a"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Step 8: Model Evaluation\n\nWe'll evaluate the trained model using:\n- Classification report (Precision, Recall, F1)\n- Confusion matrix (TP, TN, FP, FN)\n- ROC-AUC curve (performance trade-off)\n","metadata":{"id":"bT75JuQM7ZyO"}},{"cell_type":"markdown","source":"## 8.1 Load Test Set","metadata":{"id":"Irkfd6fJ7g0l"}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nfrom tensorflow.keras.metrics import AUC\n\nmodel = load_model(\"best_final_model.h5\", compile=False)\n\n# Compile again with metrics\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy', AUC(name='auc')])\n","metadata":{"id":"SRwq9K5Q7eyM"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntest_aug = ImageDataGenerator(rescale=1./255)\n\ntest_dir = \"/kaggle/input/chest-xray-pneumonia/chest_xray/test\"\n\ntest_gen = test_aug.flow_from_directory(\n    test_dir,\n    target_size=(224, 224),\n    class_mode='binary',\n    batch_size=32,\n    shuffle=False\n)\n","metadata":{"id":"aSE_WV2OWAWL","outputId":"a50ec477-fffd-4f5c-d2e7-9301162b5579"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  8.2 Load Best Model and Predict","metadata":{"id":"cEqMs1RO7k7E"}},{"cell_type":"code","source":"import numpy as np\n\n# Predict probabilities and convert to binary\npred_probs = model.predict(test_gen, verbose=1)\npreds = (pred_probs > 0.5).astype(\"int32\").flatten()\n\ntrue_labels = test_gen.classes\n","metadata":{"id":"mCgM5ILE7mYZ","outputId":"b10509f2-8bb8-4979-9afd-03d8388d5531"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n\n# Accuracy\nacc = accuracy_score(true_labels, preds)\n\n# AUC\nauc_score = roc_auc_score(true_labels, pred_probs)\n\n# Classification Report\nreport = classification_report(true_labels, preds, target_names=['NORMAL', 'PNEUMONIA'])\n\n# Confusion Matrix\ncm = confusion_matrix(true_labels, preds)\n\nprint(\"âœ… Test Accuracy:\", acc)\nprint(\"âœ… Test AUC:\", auc_score)\nprint(\"\\nðŸ“„ Classification Report:\\n\", report)\nprint(\"ðŸ”€ Confusion Matrix:\\n\", cm)\n","metadata":{"id":"l5va_drpMzVl","outputId":"e0777038-fbed-4009-de7e-8b82c30328f4"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8.3 Classification Report","metadata":{"id":"aw-aaK517wqI"}},{"cell_type":"markdown","source":"## 8.4 Confusion Matrix","metadata":{"id":"66HBwpv97zWf"}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['NORMAL', 'PNEUMONIA'], yticklabels=['NORMAL', 'PNEUMONIA'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n","metadata":{"id":"BQ3eoSbd70UF","outputId":"38165d62-87a4-4fae-e58e-e7ddfda74446"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Accuracy_score\n","metadata":{"id":"4op3xP-xM-kI"}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ntest_acc = accuracy_score(true_labels, preds)\nprint(f\"âœ… Test Accuracy: {test_acc:.4f}\")\n","metadata":{"id":"jautsxzIM7Z1","outputId":"535c6445-ba81-4b2a-fc0f-6ab0bfb7aa68"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8.5 ROC Curve & AUC","metadata":{"id":"MOBmuFPK72Ni"}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\n# Get False Positive Rate, True Positive Rate\nfpr, tpr, thresholds = roc_curve(true_labels, pred_probs)\nroc_auc = auc(fpr, tpr)\n\n# Plot ROC\nplt.figure(figsize=(7, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.4f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC)')\nplt.legend(loc=\"lower right\")\nplt.grid(alpha=0.3)\nplt.show()\n","metadata":{"id":"vjAHJhFe74Rf","outputId":"81530813-77f9-4561-857e-075ca538e681"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- High Recall = better pneumonia detection (catching most cases)\n- Confusion Matrix reveals misclassified X-rays\n- AUC closer to 1 = better discrimination between Normal and Pneumonia\n","metadata":{"id":"g9OYlBUJ75m9"}},{"cell_type":"markdown","source":"## ðŸ” Step 9: Explainability with Grad-CAM\n\nWe will apply Grad-CAM to:\n- Visualize which X-ray regions influenced the model's \"Pneumonia\" prediction.\n- Help radiologists & practitioners trust the model.\n","metadata":{"id":"V7vFMXPG-c1o"}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\n\ndef load_image(img_path, target_size=(224, 224)):\n    img = image.load_img(img_path, target_size=target_size)\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    return img, img_array / 255.0  # Normalize\n","metadata":{"id":"TMU_K-nz76K8"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  9.2 Grad-CAM Utility Function","metadata":{"id":"VHHMBrv8-fzZ"}},{"cell_type":"code","source":"def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    grad_model = tf.keras.models.Model(\n        [model.inputs],\n        [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(predictions[0])\n        class_channel = predictions[:, pred_index]\n\n    grads = tape.gradient(class_channel, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    conv_outputs = conv_outputs[0]\n    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # Normalize heatmap to [0, 1]\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n","metadata":{"id":"TThmm9k0-g89"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  9.3 Overlay Heatmap on Original X-ray","metadata":{"id":"3bttDNkU-iKe"}},{"cell_type":"code","source":"def display_gradcam(img, heatmap, alpha=0.4):\n    img = np.array(img)\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n    superimposed_img = heatmap_colored * alpha + img\n    plt.figure(figsize=(8, 6))\n    plt.imshow(superimposed_img.astype('uint8'))\n    plt.axis('off')\n    plt.title(\"Grad-CAM Visualization\")\n    plt.show()\n","metadata":{"id":"Xigbg6HZ-jYO"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  9.4 Run Grad-CAM on a Sample X-ray","metadata":{"id":"QoQEiQCG-knR"}},{"cell_type":"code","source":"# ðŸ” Input your image path here\nimage_path = \"/kaggle/input/chest-xray-pneumonia/chest_xray/test/NORMAL/IM-0037-0001.jpeg\"\n\n# 1. Load image\nimg, img_array = load_image(image_path)\n\n# 2. Predict\npred = model.predict(img_array)[0][0]\nlabel = \"PNEUMONIA\" if pred > 0.5 else \"NORMAL\"\nconfidence = pred if pred > 0.5 else 1 - pred\nprint(f\"Prediction: {label} ({confidence*100:.2f}%)\")\n\n# 3. Grad-CAM\nlast_conv_layer = \"Conv_1\"  # Last conv layer in MobileNetV2\nheatmap = make_gradcam_heatmap(img_array, model, last_conv_layer)\n\n# 4. Display\ndisplay_gradcam(img, heatmap)\n","metadata":{"id":"mPm76x3J-l2z","outputId":"eef22115-c746-4e87-f01d-4c142ef2c79c"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"âœ… Interpretation:\n- The highlighted red/yellow regions show where the model detected pneumonia.\n- Useful for radiologists and model verification.\n","metadata":{"id":"lB3t6zDC-nhD"}}]}